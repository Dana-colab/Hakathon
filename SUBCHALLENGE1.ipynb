{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -- coding: utf-8 --\n",
        "\"\"\"\n",
        "Agentic RAG Pipeline for Completion/Workover Reports (Offline)\n",
        "--------------------------------------------------------------\n",
        "- PDF -> text extraction (PyPDF2 -> pdfminer fallback)\n",
        "- Chunking + TF-IDF retrieval\n",
        "- Rule-based field extraction (well data, depths, HSE, tests)\n",
        "- Validation & sanity checks\n",
        "- Nodal analysis interface (stub) with JSON inputs\n",
        "- Word-bounded summary generation\n",
        "- Structured JSON + Markdown outputs\n",
        "- (NEW) Optional Markdown -> PDF export via --export-pdf\n",
        "\n",
        "Usage:\n",
        "  python agentic_rag_pipeline.py --pdf \"/path/to/report.pdf\" \\\n",
        "      --outdir \"/path/to/out\" \\\n",
        "      --word-limit 250 \\\n",
        "      --nodal-json \"/path/to/nodal_inputs.json\" \\\n",
        "      --export-pdf\n",
        "\n",
        "Nodal JSON schema example:\n",
        "{\n",
        "  \"wellhead_pressure_bar\": 18.0,\n",
        "  \"flow_rate_m3_h\": 135.0,\n",
        "  \"tubing_inner_diameter_in\": 6.2,\n",
        "  \"fluid_density_kg_m3\": 1015.0,\n",
        "  \"fluid_viscosity_cP\": 0.78,\n",
        "  \"reservoir_temperature_c\": 90.0\n",
        "}\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "ax8K8lH5K28Q",
        "outputId": "cf73ded0-d525-41ec-faa9-307a86b91592"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nAgentic RAG Pipeline for Completion/Workover Reports (Offline)\\n--------------------------------------------------------------\\n- PDF -> text extraction (PyPDF2 -> pdfminer fallback)\\n- Chunking + TF-IDF retrieval\\n- Rule-based field extraction (well data, depths, HSE, tests)\\n- Validation & sanity checks\\n- Nodal analysis interface (stub) with JSON inputs\\n- Word-bounded summary generation\\n- Structured JSON + Markdown outputs\\n- (NEW) Optional Markdown -> PDF export via --export-pdf\\n\\nUsage:\\n  python agentic_rag_pipeline.py --pdf \"/path/to/report.pdf\"       --outdir \"/path/to/out\"       --word-limit 250       --nodal-json \"/path/to/nodal_inputs.json\"       --export-pdf\\n\\nNodal JSON schema example:\\n{\\n  \"wellhead_pressure_bar\": 18.0,\\n  \"flow_rate_m3_h\": 135.0,\\n  \"tubing_inner_diameter_in\": 6.2,\\n  \"fluid_density_kg_m3\": 1015.0,\\n  \"fluid_viscosity_cP\": 0.78,\\n  \"reservoir_temperature_c\": 90.0\\n}\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Imports ------------------\n",
        "import os, re, json, math, argparse, sys\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any, Tuple"
      ],
      "metadata": {
        "id": "4zDCAzO2Lsk8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Helpers ------------------\n",
        "def clean_spaces(s: str) -> str:\n",
        "    s = re.sub(r\"[ \\t]+\", \" \", s or \"\")\n",
        "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
        "    return s.strip()\n",
        "\n",
        "def enforce_word_limit(s: str, limit: int) -> str:\n",
        "    if not s:  # Handle None or empty string input\n",
        "        return \"\"\n",
        "    words = re.findall(r\"\\S+\", s or \"\")\n",
        "    return \" \".join(words[:max(0, int(limit))])"
      ],
      "metadata": {
        "id": "Xy9f4EbuLqBM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ PDF Extraction ------------------\n",
        "def extract_pdf_text(pdf_path):\n",
        "    \"\"\"\n",
        "    Robust PDF text extraction:\n",
        "    1) Try pypdf (fast)\n",
        "    2) Try pdfminer.six (more thorough)\n",
        "    3) Fallback to OCR (pytesseract + pdf2image) if still empty\n",
        "    Returns a UTF-8 string (may be long).\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "\n",
        "    # 1) pypdf\n",
        "    try:\n",
        "        import pypdf\n",
        "        reader = pypdf.PdfReader(pdf_path)\n",
        "        pages = []\n",
        "        for i, p in enumerate(reader.pages):\n",
        "            pages.append(p.extract_text() or \"\")\n",
        "        text = \"\\n\".join(pages)\n",
        "        if text and text.strip():\n",
        "            return text\n",
        "    except Exception as e:\n",
        "        print(f\"[extract_pdf_text] pypdf failed: {e}\", file=sys.stderr)\n",
        "\n",
        "    # 2) pdfminer.six\n",
        "    try:\n",
        "        from pdfminer.high_level import extract_text as pdfminer_extract_text\n",
        "        t2 = pdfminer_extract_text(pdf_path) or \"\"\n",
        "        if len(t2.strip()) > len(text.strip()):\n",
        "            text = t2\n",
        "        if text and text.strip():\n",
        "            return text\n",
        "    except Exception as e:\n",
        "        print(f\"[extract_pdf_text] pdfminer failed: {e}\", file=sys.stderr)\n",
        "\n",
        "    # 3) OCR fallback\n",
        "    print(\"[extract_pdf_text] Falling back to OCR (this may take a while).\", file=sys.stderr)\n",
        "    try:\n",
        "        # Dependencies: poppler, tesseract-ocr, python libs\n",
        "        # In Colab, run once if needed:\n",
        "        # !apt-get -y install poppler-utils tesseract-ocr\n",
        "        # !pip install pdf2image pytesseract Pillow\n",
        "        from pdf2image import convert_from_path\n",
        "        import pytesseract\n",
        "        from PIL import Image\n",
        "\n",
        "        # Render images from the PDF (adjust dpi for quality/speed)\n",
        "        images = convert_from_path(pdf_path, dpi=300)\n",
        "        ocr_texts = []\n",
        "        for idx, img in enumerate(images):\n",
        "            # Optional: small preprocessing can help OCR\n",
        "            if img.mode != \"L\":\n",
        "                img = img.convert(\"L\")\n",
        "            ocr_text = pytesseract.image_to_string(img, lang=\"eng\")\n",
        "            if ocr_text:\n",
        "                ocr_texts.append(ocr_text)\n",
        "        text = \"\\n\\n\".join(ocr_texts)\n",
        "        if text and text.strip():\n",
        "            print(f\"[extract_pdf_text] OCR recovered ~{len(text)} characters.\")\n",
        "            return text\n",
        "    except Exception as e:\n",
        "        print(f\"[extract_pdf_text] OCR failed: {e}\", file=sys.stderr)\n",
        "\n",
        "    # If all failed, return empty string (caller will handle it)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "Ssy_3S32LnNU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Chunking ------------------\n",
        "def chunk_text(t: str, chunk_size=1500, overlap=300) -> List[str]:\n",
        "    if not t:\n",
        "        return []\n",
        "    res = []\n",
        "    i = 0\n",
        "    L = len(t)\n",
        "    while i < L:\n",
        "        j = min(L, i + chunk_size)\n",
        "        res.append(t[i:j])\n",
        "        i += (chunk_size - overlap)\n",
        "        if i <= 0: break\n",
        "    return res"
      ],
      "metadata": {
        "id": "VmhzgL9iLlOl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Retrieval ------------------\n",
        "def build_retriever(chunks: List[str]):\n",
        "    try:\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        from sklearn.metrics.pairwise import cosine_similarity\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\"scikit-learn is required for TF-IDF retrieval.\")\n",
        "    vectorizer = TfidfVectorizer(max_features=20000, ngram_range=(1,2))\n",
        "    tfidf = vectorizer.fit_transform(chunks) if chunks else None\n",
        "    def retrieve(query: str, k=5) -> List[Tuple[str, float]]:\n",
        "        if tfidf is None or not chunks:\n",
        "            return []\n",
        "        qv = vectorizer.transform([query])\n",
        "        sims = cosine_similarity(qv, tfidf)[0]\n",
        "        idxs = sims.argsort()[::-1][:k]\n",
        "        return [(chunks[i], float(sims[i])) for i in idxs]\n",
        "    return retrieve"
      ],
      "metadata": {
        "id": "VASTRL2JLjbU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Field Extraction ------------------\n",
        "def first_group(pattern: str, text: str, flags=re.IGNORECASE) -> str:\n",
        "    m = re.search(pattern, text, flags)\n",
        "    return m.group(1).strip() if m else \"\"\n",
        "\n",
        "def parse_report_fields(text: str) -> Dict[str, Any]:\n",
        "    d: Dict[str, Any] = {}\n",
        "    d[\"well_name\"] = first_group(r\"Well Name\\s+([^\\n]+)\", text)\n",
        "    d[\"operation\"] = first_group(r\"Operation\\s+([^\\n]+)\", text)\n",
        "    d[\"start_of_operation\"] = first_group(r\"Start of Operation\\s+([^\\n]+)\", text)\n",
        "    d[\"duration\"] = first_group(r\"Duration\\s+([^\\n]+)\", text)\n",
        "    d[\"total_depth\"] = first_group(r\"Well Total Depth\\s+([^\\n]+)\", text)\n",
        "\n",
        "    # Key events/depths\n",
        "    d[\"packer_set_depth_m\"] = first_group(r\"Set\\s+9\\s*5/8[â\\\"]?\\s+.*?at\\s+([0-9\\.]+\\s*m\\s*AHGL)\", text)\n",
        "    if not d[\"packer_set_depth_m\"]:\n",
        "        d[\"packer_set_depth_m\"] = first_group(r\"Set\\s+9\\s*5/8[â\\\"]?\\s+NOV liner hanger.*?at\\s+([0-9\\.]+\\s*m\\s*AHGL)\", text)\n",
        "\n",
        "    d[\"pbr_bottom_m\"] = first_group(r\"mule shoe at\\s+([0-9\\.]+\\s*m)\\s*AHB?GL\", text) \\\n",
        "                        or first_group(r\"bottom of (?:the )?PBR.*?([0-9\\.]+\\s*m\\s*AHGL)\", text)\n",
        "\n",
        "    d[\"hand_over\"] = first_group(r\"handed.*?to Operations on\\s+([^\\n]+)\", text)\n",
        "\n",
        "    # HSE and equipment\n",
        "    d[\"hse_incidents\"] = \"None\" if re.search(r\"No incidents\", text, re.IGNORECASE) else \"\"\n",
        "    d[\"esp_installed\"] = bool(re.search(r\"\\bESP\\b\", text))\n",
        "    d[\"gre_string\"] = bool(re.search(r\"\\bGRE\\b\", text))\n",
        "\n",
        "    # Logging/testing\n",
        "    d[\"mti_logged\"] = bool(re.search(r\"\\bMTI\\b\", text))\n",
        "    d[\"press_test_annulus\"] = \"10 bar\" if re.search(r\"Pressure tested annulus to 10 bar\", text, re.IGNORECASE) else \"\"\n",
        "\n",
        "    # Reservoir\n",
        "    d[\"reservoir_fluid\"] = \"Brine\" if re.search(r\"Well Bore Fluids:\\s*o\\s*Brine\", text, re.IGNORECASE) else \"\"\n",
        "    d[\"reservoir_bottomhole_temp_c\"] = first_group(r\"Bottom Hole temperature[:\\s]*([0-9]+)\\s*Â°C\", text)\n",
        "    return d"
      ],
      "metadata": {
        "id": "wickC3l6LhQd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Validation ------------------\n",
        "def parse_depth_m(val: str) -> float:\n",
        "    if not val:\n",
        "        return math.nan\n",
        "    m = re.search(r\"([0-9]+(?:\\.[0-9]+)?)\\s*m\", val.replace(\",\", \".\"))\n",
        "    return float(m.group(1)) if m else math.nan\n",
        "\n",
        "def validate_fields(data: Dict[str, Any]) -> List[str]:\n",
        "    issues: List[str] = []\n",
        "    depth_fields = { \"packer_set_depth_m\": data.get(\"packer_set_depth_m\", \"\"),\n",
        "                     \"pbr_bottom_m\": data.get(\"pbr_bottom_m\", \"\") }\n",
        "    for k, v in depth_fields.items():\n",
        "        d = parse_depth_m(v)\n",
        "        if math.isnan(d):\n",
        "            issues.append(f\"Missing or unparsable depth for {k}.\")\n",
        "        elif not (0 < d < 5000):\n",
        "            issues.append(f\"Unusual depth for {k}: {v}\")\n",
        "\n",
        "    if not data.get(\"start_of_operation\"):\n",
        "        issues.append(\"Start of Operation date not found.\")\n",
        "    if not data.get(\"hand_over\"):\n",
        "        issues.append(\"Hand-over to Operations date not found.\")\n",
        "    return issues\n"
      ],
      "metadata": {
        "id": "8su81gaaLeAk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Nodal Analysis Stub ------------------\n",
        "def nodal_default_inputs(data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    return {\n",
        "        \"wellhead_pressure_bar\": 10,\n",
        "        \"flow_rate_m3_h\": 50,\n",
        "        \"tubing_inner_diameter_in\": 2,\n",
        "        \"fluid_density_kg_m3\": 900,\n",
        "        \"fluid_viscosity_cP\": 0.01,\n",
        "        \"reservoir_temperature_c\": (data.get(\"reservoir_bottomhole_temp_c\") or None)\n",
        "    }\n",
        "\n",
        "def run_nodal(inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    missing = [k for k, v in inputs.items() if v in (None, \"\", float('nan'))]\n",
        "    if missing:\n",
        "        return {\n",
        "            \"status\": \"pending_inputs\",\n",
        "            \"missing_inputs\": missing,\n",
        "            \"message\": \"Provide missing nodal inputs to compute system curve and operating point.\",\n",
        "            \"results\": None\n",
        "        }\n",
        "    # Placeholder: echo operating point\n",
        "    return {\n",
        "        \"status\": \"ok\",\n",
        "        \"missing_inputs\": [],\n",
        "        \"message\": \"Computed operating point (placeholder).\",\n",
        "        \"results\": {\n",
        "            \"q_m3_h\": inputs[\"flow_rate_m3_h\"],\n",
        "            \"whp_bar\": inputs[\"wellhead_pressure_bar\"],\n",
        "            \"tubing_id_in\": inputs[\"tubing_inner_diameter_in\"]\n",
        "        }\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "kNZc98Y1LZls"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Summary Generation ------------------\n",
        "def generate_summary(data: Dict[str, Any], retrieve_func, word_limit: int) -> str:\n",
        "    highlights = []\n",
        "    if data.get(\"well_name\"): highlights.append(f\"Well: {data['well_name']}.\")\n",
        "    if data.get(\"operation\"): highlights.append(f\"Operation: {data['operation']}.\")\n",
        "    if data.get(\"start_of_operation\"): highlights.append(f\"Start: {data['start_of_operation']}.\")\n",
        "    if data.get(\"duration\"): highlights.append(f\"Duration: {data['duration']}.\")\n",
        "    if data.get(\"hand_over\"): highlights.append(f\"Handover: {data['hand_over']}.\")\n",
        "    if data.get(\"packer_set_depth_m\"): highlights.append(f\"Liner hanger/packer set at {data['packer_set_depth_m']}.\")\n",
        "    if data.get(\"pbr_bottom_m\"): highlights.append(f\"PBR reference near {data['pbr_bottom_m']}.\")\n",
        "    if data.get(\"esp_installed\"): highlights.append(\"ESP installed.\")\n",
        "    if data.get(\"mti_logged\"): highlights.append(\"MTI logging completed; annulus pressure test to 10 bar passed.\")\n",
        "    if data.get(\"hse_incidents\") == \"None\": highlights.append(\"HSE: No incidents reported; drills/toolboxes conducted.\")\n",
        "    if data.get(\"reservoir_fluid\"): highlights.append(f\"Reservoir fluid: {data['reservoir_fluid']}.\")\n",
        "    if data.get(\"reservoir_bottomhole_temp_c\"): highlights.append(f\"Bottomhole temperature: {data['reservoir_bottomhole_temp_c']} Â°C.\")\n",
        "\n",
        "    support = []\n",
        "    if retrieve_func:\n",
        "        for q in [\"Executive summary objectives outcomes\",\n",
        "                  \"Daily operations key events\",\n",
        "                  \"HSE performance incidents drills\",\n",
        "                  \"Logging MTI annulus pressure test\",\n",
        "                  \"Well data casing GRE PBR ESP depths\"]:\n",
        "            for chunk, score in retrieve_func(q, k=1):\n",
        "                # pick first sentence\n",
        "                sents = re.split(r'(?<=[\\.\\?\\!])\\s+', chunk)\n",
        "                if sents:\n",
        "                    support.append(sents[0].strip())\n",
        "\n",
        "    text = \" \".join(highlights + support)\n",
        "    return enforce_word_limit(text, word_limit)"
      ],
      "metadata": {
        "id": "uWFV8FIdLVN8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get -y install poppler-utils tesseract-ocr\n",
        "!pip install pdf2image pytesseract Pillow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsvcUmcQfjnI",
        "outputId": "0dc8c14f-a392-4595-beaa-717ba800b818"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 697 kB of additional disk space will be used.\n",
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.12 [186 kB]\n",
            "Fetched 186 kB in 1s (251 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 125082 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.12_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.12) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.12) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (25.0)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract, pdf2image\n",
            "Successfully installed pdf2image-1.17.0 pytesseract-0.3.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Markdown -> PDF Export ------------------\n",
        "def export_md_to_pdf(md_path, pdf_path):\n",
        "    \"\"\"\n",
        "    Robust Markdown -> PDF using reportlab.\n",
        "    If HTML parsing yields no content, falls back to raw Markdown text.\n",
        "    Returns True on success, False otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from reportlab.lib.pagesizes import A4\n",
        "        from reportlab.lib.units import mm\n",
        "        from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "        from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, ListFlowable, ListItem\n",
        "        from reportlab.lib.enums import TA_LEFT\n",
        "        from reportlab.lib import utils\n",
        "        import markdown\n",
        "        from bs4 import BeautifulSoup\n",
        "    except Exception:\n",
        "        print(\"PDF export requires: reportlab, markdown, beautifulsoup4.\", file=sys.stderr)\n",
        "        print(\"In Colab, run:  !pip install reportlab markdown beautifulsoup4\", file=sys.stderr)\n",
        "        return False\n",
        "\n",
        "    # Read MD\n",
        "    with open(md_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        md_text = f.read()\n",
        "    if not md_text.strip():\n",
        "        print(\"Markdown file is empty; writing placeholder page.\", file=sys.stderr)\n",
        "        md_text = \"# Completion Report Summary\\n\\n(No content generated.)\"\n",
        "\n",
        "    # Convert to HTML\n",
        "    html = markdown.markdown(md_text, extensions=[\"tables\", \"fenced_code\", \"sane_lists\"])\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "    # Styles\n",
        "    styles = getSampleStyleSheet()\n",
        "    if \"Heading1L\" not in styles:\n",
        "        styles.add(ParagraphStyle(name=\"Heading1L\", parent=styles[\"Heading1\"], alignment=TA_LEFT, spaceAfter=8))\n",
        "    if \"Heading2L\" not in styles:\n",
        "        styles.add(ParagraphStyle(name=\"Heading2L\", parent=styles[\"Heading2\"], alignment=TA_LEFT, spaceAfter=6))\n",
        "    if \"BodyL\" not in styles:\n",
        "        styles.add(ParagraphStyle(name=\"BodyL\", parent=styles[\"BodyText\"], spaceAfter=6, leading=14))\n",
        "\n",
        "    story = []\n",
        "\n",
        "    def para(text, style=\"BodyL\", space=4):\n",
        "        story.append(Paragraph(text, styles[style]))\n",
        "        story.append(Spacer(1, space))\n",
        "\n",
        "    def handle_list(tag):\n",
        "        items = []\n",
        "        for li in tag.find_all(\"li\", recursive=False):\n",
        "            items.append(ListItem(Paragraph(li.decode_contents() or \"\", styles[\"BodyL\"])))\n",
        "        if items:\n",
        "            story.append(ListFlowable(items, bulletType=\"bullet\" if tag.name == \"ul\" else \"1\"))\n",
        "            story.append(Spacer(1, 4))\n",
        "\n",
        "    # Build flowables from top-level nodes\n",
        "    recognized = 0\n",
        "    for node in soup.children:\n",
        "        name = getattr(node, \"name\", None)\n",
        "        if name == \"h1\":\n",
        "            para(node.decode_contents(), \"Heading1L\", 8); recognized += 1\n",
        "        elif name == \"h2\":\n",
        "            para(node.decode_contents(), \"Heading2L\", 6); recognized += 1\n",
        "        elif name in {\"p\", \"pre\", \"code\", \"h3\"}:\n",
        "            para(node.decode_contents(), \"BodyL\", 4); recognized += 1\n",
        "        elif name in {\"ul\", \"ol\"}:\n",
        "            handle_list(node); recognized += 1\n",
        "        elif name == \"table\":\n",
        "            # Very simple table handling: flatten to text\n",
        "            text = node.get_text(separator=\"  |  \").strip()\n",
        "            if text:\n",
        "                para(text, \"BodyL\", 4); recognized += 1\n",
        "        else:\n",
        "            # Raw text nodes\n",
        "            raw = str(node).strip()\n",
        "            if raw:\n",
        "                para(raw, \"BodyL\", 4); recognized += 1\n",
        "\n",
        "    # Fallback if nothing was recognized: write raw MD as a paragraph\n",
        "    if not story:\n",
        "        para(\"## Completion Report Summary (raw markdown fallback)\", \"Heading2L\", 8)\n",
        "        # Escape angle brackets for Paragraph\n",
        "        safe_md = md_text.replace(\"&\", \"&amp;\").replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n",
        "        # Replace newlines with <br/> to preserve structure\n",
        "        safe_md = safe_md.replace(\"\\n\", \"<br/>\")\n",
        "        para(safe_md, \"BodyL\", 4)\n",
        "\n",
        "    # Build PDF\n",
        "    doc = SimpleDocTemplate(\n",
        "        pdf_path, pagesize=A4,\n",
        "        leftMargin=18*mm, rightMargin=18*mm,\n",
        "        topMargin=18*mm, bottomMargin=18*mm\n",
        "    )\n",
        "    try:\n",
        "        doc.build(story)\n",
        "    except Exception as e:\n",
        "        print(f\"PDF build failed: {e}\", file=sys.stderr)\n",
        "        return False\n",
        "\n",
        "    # Quick debug\n",
        "    print(f\"[export_md_to_pdf] nodes_parsed={recognized} flowables={len(story)}\")\n",
        "    return os.path.exists(pdf_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "aPvoDuw3LQAP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install reportlab markdown beautifulsoup4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pi2ecmBdZoaF",
        "outputId": "053d3a42-1389-469a-bc33-7a2462d97bf0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting reportlab\n",
            "  Downloading reportlab-4.4.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.12/dist-packages (3.10)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from reportlab) (11.3.0)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from reportlab) (3.4.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Downloading reportlab-4.4.4-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: reportlab\n",
            "Successfully installed reportlab-4.4.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse, json, os, re, sys\n",
        "from datetime import datetime\n",
        "\n",
        "# Detect notebook so we don't auto-run main() in Colab/Jupyter\n",
        "IN_NOTEBOOK = any(m in sys.modules for m in (\"ipykernel\", \"google.colab\"))\n",
        "\n",
        "def run(pdf, outdir=\".\", word_limit=250, nodal_json=None, export_pdf=False):\n",
        "    if not os.path.exists(pdf):\n",
        "        raise FileNotFoundError(f\"PDF not found: {pdf}\")\n",
        "    if word_limit < 1:\n",
        "        word_limit = 250\n",
        "\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "    # ---- Extraction & preprocessing ----\n",
        "    raw = extract_pdf_text(pdf)\n",
        "    text = clean_spaces(raw)\n",
        "    chunks = chunk_text(text, chunk_size=1500, overlap=300)\n",
        "\n",
        "    retrieve_func = None\n",
        "    if chunks:\n",
        "        try:\n",
        "            retrieve_func = build_retriever(chunks)\n",
        "        except Exception:\n",
        "            retrieve_func = None\n",
        "\n",
        "    # ---- Parsing & validation ----\n",
        "    extracted = parse_report_fields(text)\n",
        "    issues = validate_fields(extracted)\n",
        "\n",
        "    # ---- Nodal inputs ----\n",
        "    nodal_inputs = nodal_default_inputs(extracted)\n",
        "    if nodal_json and os.path.exists(nodal_json):\n",
        "        with open(nodal_json, \"r\", encoding=\"utf-8\") as f:\n",
        "            nodal_inputs.update(json.load(f))\n",
        "\n",
        "    # ---- Questions & summary ----\n",
        "    questions = []\n",
        "    if run_nodal(nodal_inputs).get(\"status\") != \"ok\":\n",
        "        for m in run_nodal(nodal_inputs).get(\"missing_inputs\", []):\n",
        "            questions.append(f\"Please provide *{m.replace('_',' ').capitalize()}*.\")\n",
        "\n",
        "    base_summary = generate_summary(extracted, retrieve_func, word_limit)\n",
        "\n",
        "    if run_nodal(nodal_inputs).get(\"status\") == \"ok\":\n",
        "        r = run_nodal(nodal_inputs)[\"results\"]\n",
        "        try:\n",
        "            nodal_line = (\n",
        "                f\" Nodal operating point (stub): q ≈ {r['q_m3_h']} m³/h at \"\n",
        "                f\"WHP ≈ {r['whp_bar']} bar (Tubing ID {r['tubing_id_in']} in).\"\n",
        "            )\n",
        "        except Exception:\n",
        "            nodal_line = \" Nodal operating point (stub): results available but incomplete.\"\n",
        "        base_summary = enforce_word_limit(base_summary + nodal_line, word_limit)\n",
        "\n",
        "    out_json = {\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"inputs\": {\"pdf\": os.path.basename(pdf), \"word_limit\": word_limit},\n",
        "        \"data_extracted\": extracted,\n",
        "        \"validation_issues\": issues,\n",
        "        \"nodal_inputs_used\": nodal_inputs,\n",
        "        \"nodal_status\": run_nodal(nodal_inputs),\n",
        "        \"questions_for_user\": questions,\n",
        "        \"summary_words\": len(re.findall(r\"\\S+\", base_summary)),\n",
        "        \"summary\": base_summary,\n",
        "    }\n",
        "\n",
        "    json_path = os.path.join(outdir, \"rag_agentic_outputs.json\")\n",
        "    md_path = os.path.join(outdir, \"rag_agentic_summary.md\")\n",
        "\n",
        "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(out_json, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"# Completion Report Summary (≤{word_limit} words)\\n\\n\")\n",
        "        f.write(base_summary + \"\\n\")\n",
        "        if questions:\n",
        "            f.write(\"\\n## Missing Inputs for Nodal Analysis\\n\")\n",
        "            for q in questions:\n",
        "                f.write(f\"- {q}\\n\")\n",
        "\n",
        "        if export_pdf:\n",
        "            pdf_path = os.path.join(outdir, \"rag_agentic_summary.pdf\")\n",
        "            ok = export_md_to_pdf(md_path, pdf_path)\n",
        "        if ok:\n",
        "            print(\"Exported PDF:\", pdf_path)\n",
        "            # Auto-offer download in Colab\n",
        "            if IN_NOTEBOOK and \"google.colab\" in sys.modules:\n",
        "                try:\n",
        "                    from google.colab import files\n",
        "                    files.download(pdf_path)\n",
        "                except Exception as e:\n",
        "                    print(f\"(Could not auto-download PDF: {e})\", file=sys.stderr)\n",
        "        else:\n",
        "            print(\"PDF export failed or missing deps. In Colab run:\", file=sys.stderr)\n",
        "            print(\"!pip install reportlab markdown beautifulsoup4\", file=sys.stderr)\n",
        "\n",
        "# Do NOT auto-run main() inside notebooks\n",
        "if __name__ == \"__main__\" and not IN_NOTEBOOK:\n",
        "    main()\n",
        "\n",
        "run(\n",
        "    pdf= \"/content/TNO-Report-2015-R10065-final-public2020.pdf\",\n",
        "    outdir=\"/content/out\",\n",
        "    word_limit=250,\n",
        "    nodal_json=\"/content/nodal.json\",  # or None\n",
        "    export_pdf=True\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "RqPYhkXhPbEM",
        "outputId": "0724b000-08e9-46cd-e7eb-cd02e9ce8588"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[extract_pdf_text] pypdf failed: No module named 'pypdf'\n",
            "[extract_pdf_text] pdfminer failed: No module named 'pdfminer'\n",
            "[extract_pdf_text] Falling back to OCR (this may take a while).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[extract_pdf_text] OCR recovered ~190421 characters.\n",
            "[export_md_to_pdf] nodes_parsed=2 flowables=0\n",
            "Exported PDF: /content/out/rag_agentic_summary.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Markdown file is empty; writing placeholder page.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3abcedb0-1e46-4364-8323-46140cf75d1d\", \"rag_agentic_summary.pdf\", 1665)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "c-suwc1mhxlm"
      }
    }
  ]
}